<!doctype html>
<html lang="en" dir="ltr" class="mdx-wrapper mdx-page plugin-pages plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Research | Lotfollahi</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://demo.lotfollahi.com/research"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Research | Lotfollahi"><meta data-rh="true" name="description" content="Overview"><meta data-rh="true" property="og:description" content="Overview"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://demo.lotfollahi.com/research"><link data-rh="true" rel="alternate" href="https://demo.lotfollahi.com/research" hreflang="en"><link data-rh="true" rel="alternate" href="https://demo.lotfollahi.com/research" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Lotfollahi RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Lotfollahi Atom Feed"><link rel="stylesheet" href="/assets/css/styles.081db42e.css">
<link rel="preload" href="/assets/js/runtime~main.32c4b9b6.js" as="script">
<link rel="preload" href="/assets/js/main.920226e3.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/lotfollahiLab.jpg" alt="Lotfollahi Lab" class="themedImage_ToTc themedImage--light_HNdA" height="40" width="140"><img src="/img/lotfollahiLab.jpg" alt="Lotfollahi Lab" class="themedImage_ToTc themedImage--dark_i4oU" height="40" width="140"></div></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Home</a><a class="navbar__item navbar__link" href="/about">About</a><a class="navbar__item navbar__link" href="/Lab">The Lab</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/research">Research</a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/blog">News</a><a class="navbar__item navbar__link" href="/software">Software</a><a class="navbar__item navbar__link" href="/talks">Talks</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_j9I6"><div class="col col--8"><article><h1>Research</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2><ul><li>In this page, I highlight my research on how machine learning and computational models enable understanding, mapping, and predicting cellular biology.</li><li>Before 2018, as part of my AI program master thesis, I developed <strong><a href="https://scholar.google.com/citations?user=NXhouUcAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Deep Packet</a></strong>, the first neural network architecture for network traffic classification, and it has become one of the seminal papers in the field.</li><li>Papers selected as cover:</li></ul><div style="display:flex;justify-content:center;flex-wrap:wrap"><figure><img loading="lazy" src="/img/nature-biotech.jpeg" style="margin:10px" alt="NBT" width="300" height="300" class="img_ev3q"><figcaption> <font size="1"> <a href="https://www.nature.com/nbt/volumes/40/issues/1" target="_blank" rel="noopener noreferrer">Lotfollahi et al., Nature Biotechnology (2022)</a></font></figcaption></figure><figure><img loading="lazy" src="img/morris-lab-cpa.jpg" style="margin:10px" alt="alt text" width="300" height="300" class="img_ev3q"><figcaption> <font size="1"> <a href="https://www.embopress.org/toc/17444292/2023/19/6" target="_blank" rel="noopener noreferrer">Lotfollahi et al., Molecular Systems Biology (2023)</a></font></figcaption></figure></div><ul><li>See a full list of papers on <strong><a href="https://scholar.google.com/citations?user=NXhouUcAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a></strong>.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="generative-ai-for-modeling-single-cell-perturbation">Generative AI for Modeling Single-Cell Perturbation<a href="#generative-ai-for-modeling-single-cell-perturbation" class="hash-link" aria-label="Direct link to Generative AI for Modeling Single-Cell Perturbation" title="Direct link to Generative AI for Modeling Single-Cell Perturbation">​</a></h2><p>During my doctoral studies, I developed a series of generative AI algorithms to predict out-of-distribution cellular behaviors in response to perturbations (e.g., diseases, drugs, CRISPR KOs). Using these models, one can predict and answer counterfactual questions such as, &quot;What would the gene expression of this cell have looked like if it had been treated differently?&quot;</p><p><img loading="lazy" alt="Alt text" src="/assets/images/pert-a6631732bee2b168783df97f91aa644f.png" title="Title" width="463" height="248" class="img_ev3q"></p><p>The first approach is called the single-cell generator (<a href="https://www.nature.com/articles/s41592-019-0494-8" target="_blank" rel="noopener noreferrer">scGen</a>). It models perturbation effects using simple arithmetic in the latent space. Later, we formulated the problem as a distribution matching scenario, known as <a href="https://academic.oup.com/bioinformatics/article/36/Supplement_2/i610/6055927" target="_blank" rel="noopener noreferrer">trVAE</a>, where we aim to move cells from a control distribution to a perturbed condition.</p><p>Finally, during my time at Facebook AI, we developed the composition perturbation autoencoder (<a href="https://www.embopress.org/doi/full/10.15252/msb.202211517" target="_blank" rel="noopener noreferrer">CPA</a>), which can predict combinatorial perturbations such as drug combinations or double CRISPR KOs. We also extended CPA to predict unseen drugs (<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/aa933b5abc1be30baece1d230ec575a7-Abstract-Conference.html" target="_blank" rel="noopener noreferrer">chemCPA</a>) and to support multiple modalities (<a href="https://www.biorxiv.org/content/10.1101/2022.07.08.499049v1.abstract" target="_blank" rel="noopener noreferrer">multiCPA</a>).</p><p>In addition to the above, we have written a <a href="https://www.sciencedirect.com/science/article/pii/S2405471221002027" target="_blank" rel="noopener noreferrer">perspective</a> about the challenges and opportunities in this emerging field.</p><p><strong>scGen predicts single-cell perturbation responses.</strong></p><u>Lotfollahi, M.</u>, Wolf, F. A. &amp; Theis, F. J.<p><a href="https://doi.org/10.1038/s41592-019-0494-8" target="_blank" rel="noopener noreferrer">[<!-- -->Nature Methods (2019)<!-- -->]</a>,
<a href="https://github.com/theislab/scgen" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a>,
<a href="https://www.bioengineering.tum.de/en/news/details/ai-extrapolates-from-mice-to-humans-1" target="_blank" rel="noopener noreferrer">[<!-- -->press<!-- -->]</a>.</p><hr><p><strong>Conditional out-of-distribution generation for unpaired data using transfer VAE.</strong></p><u>Lotfollahi, M.</u>, Naghipourfar, M., Theis, F. J. &amp; Wolf, F. A.<p><a href="https://doi.org/10.1093/bioinformatics/btaa800" target="_blank" rel="noopener noreferrer">[Bioinformatics (2020)]</a>,
<a href="https://github.com/theislab/trvae" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a>,
<a href="https://eccb2020.info/" target="_blank" rel="noopener noreferrer">[<!-- -->talk at ECCB 2020 (21.18% accept rate)<!-- -->]</a>.</p><hr><p><strong>Predicting cellular responses to complex perturbations in high-throughput screens.</strong></p><u>Lotfollahi, M+.</u>, Klimovskaia Susmelj+, A., De Donno, C+., Hetzel, L., Ji, Y., Ibarra, I. L., ... &amp; Theis, F. J.<p><a href="https://www.embopress.org/doi/full/10.15252/msb.202211517" target="_blank" rel="noopener noreferrer">[Molecular Systems Biology (2023)]</a>,
<a href="https://github.com/facebookresearch/CPA" target="_blank" rel="noopener noreferrer">[code]</a>,
<a href="https://ai.facebook.com/blog/ai-predicts-effective-drug-combinations-to-fight-complex-diseases-faster" target="_blank" rel="noopener noreferrer">[Facebook AI blogpost]</a>,
<a href="https://www.stateof.ai/2021" target="_blank" rel="noopener noreferrer">[state of AI report 2021]</a>,
<a href="https://www.embopress.org/loi/17444292" target="_blank" rel="noopener noreferrer">[featured cover]</a>.</p><hr><p><strong>Predicting single-cell perturbation responses for unseen drugs.</strong></p><p>Hetzel, L., Böhm, S., Kilbertus, N., Günnemann, S., <u>Lotfollahi, M.</u>, &amp; Theis, F. (2022).</p><p><a href="https://nips.cc/Conferences/2022/ScheduleMultitrack?event=53227" target="_blank" rel="noopener noreferrer">[NeurIPS (2022)]</a>,
<a href="https://github.com/theislab/chemcpa" target="_blank" rel="noopener noreferrer">[code]</a>.</p><hr><p><strong>Machine learning for perturbational single-cell omics.</strong></p><p>Ji, Y.,<u> Lotfollahi, M.</u>, Wolf, F. A. &amp; Theis, F. J.</p><p><a href="https://doi.org/10.1016/j.cels.2021.05.016" target="_blank" rel="noopener noreferrer">[Cell Systems (2021)]</a>,
<a href="https://github.com/theislab/sc-pert" target="_blank" rel="noopener noreferrer">[data resource]</a>.</p><hr><p><strong>MultiCPA: Multimodal Compositional Perturbation Autoencoder.</strong>
Inecik, K., Uhlmann, A., <u>Lotfollahi, M.<!-- -->*</u>, &amp; Theis, F<!-- -->*<!-- -->.</p><p><a href="https://icml-compbio.github.io/icml-website-2022/" target="_blank" rel="noopener noreferrer">[ICML Workshop on Computational Biology (WCB) 2022]</a>,
<a href="https://www.biorxiv.org/content/10.1101/2022.07.08.499049v1" target="_blank" rel="noopener noreferrer">[bioRxv]</a>,
<a href="https://github.com/theislab/multicpa" target="_blank" rel="noopener noreferrer">[code]</a>.</p><hr><p><strong>Out-of-distribution prediction with disentangled representations for single-cell RNA sequencing data</strong></p><u>Lotfollahi, M.+</u>, Dony, L.+, Agarwala, H.+, &amp; Theis, F. J.<p><a href="https://icml-compbio.github.io/icml-website-2020/" target="_blank" rel="noopener noreferrer">[ICML Workshop on Computational Biology (WCB) 2020]</a>,
<a href="https://slideslive.com/38931310/outofdistribution-prediction-with-disentangled-representations-for-scrnaseq-data" target="_blank" rel="noopener noreferrer">[spotlight talk ICML WCB 2020]</a>,
<a href="https://github.com/theislab/disent" target="_blank" rel="noopener noreferrer">[code]</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="generative-ai-for-modeling-high-content-microscopy-image">Generative AI for modeling high content Microscopy image<a href="#generative-ai-for-modeling-high-content-microscopy-image" class="hash-link" aria-label="Direct link to Generative AI for modeling high content Microscopy image" title="Direct link to Generative AI for modeling high content Microscopy image">​</a></h2><p>Advancements in high-throughput screening, particularly
in high-content microscopy, have accelerated drug target identification and mode of
action studies by allowing the exploration of complex phenotypic data.
However, scaling these experiments to encompass a wide range of drug or
genetic manipulations is challenging because only a limited number
of compounds exhibit activity in screenings.
<img loading="lazy" alt="Alt text" src="/assets/images/img-0f19c660a7989137a2a1e9d753be5684.png" title="Title" width="998" height="443" class="img_ev3q"></p><p><strong>Predicting Cell Morphological Responses to Perturbations Using Generative Modeling</strong></p><p>To address this, we developed a generative model, the Image Perturbation Autoencoder (IMPA), which predicts cellular morphological effects of chemical and genetic perturbations using untreated cells as input.</p><p>Palma, A., Theis, F. J.<!-- -->*<!-- -->, <u>Lotfollahi, M.<!-- -->*</u>.</p><p><a href="https://www.biorxiv.org/content/10.1101/2023.07.17.549216v1" target="_blank" rel="noopener noreferrer">[bioRxiv (2023)]</a>,
<a href="https://github.com/theislab/impa" target="_blank" rel="noopener noreferrer">[code]</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="modeling-tissue-and-spatial-biology">Modeling Tissue and Spatial Biology<a href="#modeling-tissue-and-spatial-biology" class="hash-link" aria-label="Direct link to Modeling Tissue and Spatial Biology" title="Direct link to Modeling Tissue and Spatial Biology">​</a></h2><p><img loading="lazy" alt="Alt text" src="/assets/images/spatial-5b78e554cd02770e8f7c214b1d133e16.png" title="Title" width="944" height="648" class="img_ev3q"></p><p>Spatial omics holds great potential to elucidate tissue architecture by dissecting underlying cell niches and cellular interactions. However, we lack an end-to-end computational framework that can effectively integrate different spatial omics tissue samples, quantitatively characterize cell niches based on biological knowledge of cell-cell communication and transcriptional regulation pathways,
and discover spatial molecular programs of cells. We present NicheCompass,
a graph deep learning method designed based on the principles of cellular communication. It utilizes existing knowledge of inter- and intracellular interaction pathways to learn an interpretable latent space of cells across multiple tissue samples, enabling the construction and querying of spatial reference atlases. </p><p>Birk, S., Bonafonte-Pardàs, I., Feriz, A. M.,, ... &amp; <u>Lotfollahi, M.<!-- -->*</u>.</p><p><a href="https://github.com/lotfollahi-lab" target="_blank" rel="noopener noreferrer">[code]</a>
<a href="https://www.biorxiv.org/content/10.1101/2024.02.21.581428v2" target="_blank" rel="noopener noreferrer">[bioRxv (2024)]</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="single-cell-reference-mapping">Single-cell Reference Mapping<a href="#single-cell-reference-mapping" class="hash-link" aria-label="Direct link to Single-cell Reference Mapping" title="Direct link to Single-cell Reference Mapping">​</a></h2><p><img loading="lazy" alt="Alt text" src="/assets/images/ref-3a6aa2f93d398ecf9be329f8d4107820.png" title="Title" width="1411" height="660" class="img_ev3q"></p><p>The availability of single-cell reference datasets and mapping algorithms transforms analytical workflows for single-cell sequencing datasets. These reference atlases are generated with the intention of helping individual labs in the field understand their own data. Single-cell reference mapping addresses the question of how this can be done efficiently and in a reusable fashion, enabling information accumulated from multiple prior experiments to help interpret new data. The ultimate goal is to transition from an expert-centric and tedious pipeline to a rapid, accessible, and accurate procedure for beginners and experts alike.</p><p>We introduced the first deep learning algorithm to map single-cell datasets into pretrained reference building methods called single-cell architecture surgery (<a href="https://www.nature.com/articles/s41587-021-01001-7" target="_blank" rel="noopener noreferrer">scArches</a>). scArches receives a pretrained model and a query dataset, and maps the query data to the reference without retraining the reference model. scArches is now widely used by the community to understand disease, development, in vivo/vitro differences, imputing missing modalities, and transferring cell-type annotations from reference to query by mapping those query datasets onto a reference atlas. We later introduced <a href="https://www.biorxiv.org/content/10.1101/2022.07.07.499109v2" target="_blank" rel="noopener noreferrer">treeArches</a> to not just update the reference but also cell-type hierarchies.</p><p>We extended reference mapping to support multiple data modalities such as RNA/ATAC using <a href="https://www.biorxiv.org/content/10.1101/2022.03.16.484643v1.abstract" target="_blank" rel="noopener noreferrer">Multigrate</a>. Additionally, we developed <a href="https://www.nature.com/articles/s41556-022-01072-x" target="_blank" rel="noopener noreferrer">expiMap</a> to learn novel gene programs for query datasets. Furthermore, we improved technical aspects by leveraging continuous embeddings with <a href="https://www.google.com/search?q=scpoli" target="_blank" rel="noopener noreferrer">scPoli</a> and using continual learning strategies through <a href="https://icml-compbio.github.io/2022/papers/WCBICML2022_paper_68.pdf" target="_blank" rel="noopener noreferrer">continual surgery</a>. The <a href="https://github.com/theislab/scarches" target="_blank" rel="noopener noreferrer">scArches repository</a> now serves as a unified framework integrating many applications of single-cell reference mapping including the above.</p><p><strong>Mapping Single-cell Data to Reference Atlases by Transfer Learning</strong></p><u>Lotfollahi, M.</u>, Naghipourfar, M., Luecken, M. D., Khajavi, M., Büttner, M., Wagenstetter, M., Avsec, Ž., Gayoso, A., Yosef, N., Interlandi, M., &amp; Others.<p><a href="https://www.nature.com/articles/s41587-021-01001-7" target="_blank" rel="noopener noreferrer">[Nature Biotechnology (2022)]</a>,
<a href="https://github.com/theislab/scarches" target="_blank" rel="noopener noreferrer">[code]</a>,
<a href="https://www.mdsi.tum.de/en/mdsi/latest-info/news/full-text/article/awardees-of-the-mdsi-best-paper-of-the-year-award/" target="_blank" rel="noopener noreferrer">[MDSI best paper award]</a>,
<a href="https://www.nature.com/nbt/volumes/40/issues/1" target="_blank" rel="noopener noreferrer">[featured cover in Nature Biotechnology]</a>.</p><hr><p><strong>Single-cell Reference Mapping to Construct and Extend Cell Type Hierarchies</strong></p><p>Michielsen, L+., <u>Lotfollahi, M.+</u>, Strobl, D., Sikkema, L., Reinders, M. J. T., Theis, F. J., Mahfouz, A.</p><p><a href="https://academic.oup.com/nargab/article/5/3/lqad070/7231336?login=false" target="_blank" rel="noopener noreferrer">[NAR Genomics (2024)]</a>,
<a href="https://github.com/theislab/scarches" target="_blank" rel="noopener noreferrer">[code]</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="multimodal-modeling-of-single-cell-data">Multimodal Modeling of Single-Cell Data<a href="#multimodal-modeling-of-single-cell-data" class="hash-link" aria-label="Direct link to Multimodal Modeling of Single-Cell Data" title="Direct link to Multimodal Modeling of Single-Cell Data">​</a></h2><p>The integration and simultaneous analysis of genomic, epigenomic, transcriptomic, proteomic, and metabolomic data at the single-cell level are revolutionizing our understanding of cell biology in both normal and diseased states.</p><p><img loading="lazy" alt="Alt text" src="/assets/images/mul-3d73830f3b7e389d09f99af274d04221.png" title="Title" width="429" height="430" class="img_ev3q"></p><p>We have developed two innovative generative models to facilitate this integration:</p><ol><li><p><a href="https://www.biorxiv.org/content/10.1101/2022.03.16.484643v1.abstract" target="_blank" rel="noopener noreferrer">Multigrate</a>: This model enables the integration of partially overlapping single-cell modalities to construct a comprehensive multimodal reference atlas. It incorporates single-cell chromatin accessibility, transcriptomics, and surface protein abundance.</p></li><li><p><a href="https://www.biorxiv.org/content/10.1101/2021.06.24.449733v2.abstract" target="_blank" rel="noopener noreferrer">mvTCR</a>: This model is designed to integrate T-cell receptor sequences with single-cell RNA-seq data.</p></li></ol><p><strong>Multigrate: Single-Cell Multi-Omic Data Integration.</strong></p><u>Lotfollahi, M+</u>, Litinetskaya, A+, and Theis, F. J.<p><a href="https://icml-compbio.github.io/icml-website-2021/" target="_blank" rel="noopener noreferrer">[Contributed talk Award at ICML Workshop on Computational Biology 2021]</a>,
<a href="https://github.com/theislab/multigrate" target="_blank" rel="noopener noreferrer">[code]</a>,
<a href="https://www.biorxiv.org/content/10.1101/2022.03.16.484643v1" target="_blank" rel="noopener noreferrer">[bioRxv (2022)]</a>.</p><hr><p><strong>Integrating T-cell receptor and transcriptome for large-scale single-cell immune profiling analysis.</strong></p><p>Drost, F., An, Y., Dratva, L. M., Lindeboom, R. G. H., Haniffa, M., Teichmann, S. A., Theis, F., <u>Lotfollahi, M.<!-- -->*</u>, Schubert, B<!-- -->*<!-- -->.</p><p><a href="https://icml-compbio.github.io/icml-website-2021/2021/papers/WCBICML2021_paper_45.pdf" target="_blank" rel="noopener noreferrer">[ICML Workshop on Computational Biology 2021]</a>,
<a href="https://github.com/SchubertLab/mvTCR" target="_blank" rel="noopener noreferrer">[code]</a>,
<a href="https://www.biorxiv.org/content/10.1101/2021.06.24.449733v2.abstract" target="_blank" rel="noopener noreferrer">[bioRxv (2021)]</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="biologically-informed-deep-learning-for-single-cell-genomics">Biologically Informed Deep Learning for Single-Cell Genomics<a href="#biologically-informed-deep-learning-for-single-cell-genomics" class="hash-link" aria-label="Direct link to Biologically Informed Deep Learning for Single-Cell Genomics" title="Direct link to Biologically Informed Deep Learning for Single-Cell Genomics">​</a></h2><p>The availability of large-scale single-cell atlases has provided us with detailed insights into cell states. At the same time, advancements in deep learning have facilitated the rapid analysis of query datasets by mapping them into reference atlases. However, the existing data transformations learned by these methods lack interpretability in terms of biologically known concepts such as genes or pathways.</p><p><img loading="lazy" alt="Alt text" src="/assets/images/inter-07a57d1b33a7c07fa28cf8299d72762c.png" title="Title" width="497" height="256" class="img_ev3q"></p><p>To address this limitation, we introduced two methods: expiMap and intercode. These methods embed single-cell data within a biologically meaningful space that captures the activity of gene programs. Additionally, we demonstrate the feasibility of learning novel gene programs using expiMap.</p><p><strong>Biologically Informed Deep Learning to Query Gene Programs in Single-Cell Atlases</strong></p><u>Lotfollahi, M</u>, M+, Rybakov, S+, Hrovatin, K., Hediyeh-Zadeh, S., Talavera-López, C., Misharin, A. V., &amp; Theis, F. J.<p><a href="https://icml-compbio.github.io/icml-website-2021/2021/papers/WCBICML2021_paper_45.pdf" target="_blank" rel="noopener noreferrer">[Nature Cell Biology (2023)]</a>,
<a href="https://github.com/theislab/scArches" target="_blank" rel="noopener noreferrer">[code]</a>.</p><hr><p><strong>Learning Interpretable Latent Autoencoder Representations with Annotations of Feature Sets</strong></p><p>S. Rybakov, <u>M. Lotfollahi</u>, F.J. Theis<!-- -->*<!-- -->, F.A. Wolf<!-- -->*<!-- -->.</p><p><a href="https://doi.org/10.1101/2020.12.02.401182" target="_blank" rel="noopener noreferrer">[Machine Learning in Computational Biology (2020)]</a>,
<a href="https://github.com/theislab/intercode" target="_blank" rel="noopener noreferrer">[code]</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="population-level-integration-of-single-cell-datasets">Population-level Integration of Single-Cell Datasets<a href="#population-level-integration-of-single-cell-datasets" class="hash-link" aria-label="Direct link to Population-level Integration of Single-Cell Datasets" title="Direct link to Population-level Integration of Single-Cell Datasets">​</a></h2><p>The increasing generation of population-level single-cell atlases with hundreds or thousands of samples has the potential to link demographic and technical metadata with high-resolution cellular and tissue data in homeostasis and disease. Constructing such comprehensive references requires large-scale integration of heterogeneous cohorts with varying metadata capturing demographic and technical information.</p><p><img loading="lazy" alt="Alt text" src="/assets/images/pop-fc5d545923a7f8440908c69df35ae2cf.png" title="Title" width="384" height="372" class="img_ev3q"></p><p>We introduced scPoli, which learns both sample and cell representations, is aware of cell-type annotations, and can integrate and annotate newly generated query datasets while providing an uncertainty mechanism to identify unknown populations. It</p><p>De Donno, C., Hediyeh-Zadeh, S., Wagenstetter, M., Moinfar, A. A., Zappia, L.,</p><u> Lotfollahi, M.* </u>, &amp; Theis, F. J *.<p><a href="https://github.com/theislab/scArches" target="_blank" rel="noopener noreferrer">[code]</a>,
<a href="https://www.nature.com/articles/s41592-023-02035-2" target="_blank" rel="noopener noreferrer">[Nature Methods (2023)]</a>.</p></article></div><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#generative-ai-for-modeling-single-cell-perturbation" class="table-of-contents__link toc-highlight">Generative AI for Modeling Single-Cell Perturbation</a></li><li><a href="#generative-ai-for-modeling-high-content-microscopy-image" class="table-of-contents__link toc-highlight">Generative AI for modeling high content Microscopy image</a></li><li><a href="#modeling-tissue-and-spatial-biology" class="table-of-contents__link toc-highlight">Modeling Tissue and Spatial Biology</a></li><li><a href="#single-cell-reference-mapping" class="table-of-contents__link toc-highlight">Single-cell Reference Mapping</a></li><li><a href="#multimodal-modeling-of-single-cell-data" class="table-of-contents__link toc-highlight">Multimodal Modeling of Single-Cell Data</a></li><li><a href="#biologically-informed-deep-learning-for-single-cell-genomics" class="table-of-contents__link toc-highlight">Biologically Informed Deep Learning for Single-Cell Genomics</a></li><li><a href="#population-level-integration-of-single-cell-datasets" class="table-of-contents__link toc-highlight">Population-level Integration of Single-Cell Datasets</a></li></ul></div></div></div></main></div><footer class="footer"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Mo Lotfollahi.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.32c4b9b6.js"></script>
<script src="/assets/js/main.920226e3.js"></script>
</body>
</html>